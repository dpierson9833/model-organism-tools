{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Organisms Project - Colab Setup\n",
    "\n",
    "This notebook sets up and runs the model organisms project in Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available. Using CPU.\")\n",
    "    print(\"Go to Runtime > Change runtime type and select GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate>=1.10.1 datasets>=4.1.0 tokenizers>=0.22.0 transformers>=4.56.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clone or upload project files\n",
    "\n",
    "Option 1: If your project is on GitHub, uncomment and run:\n",
    "```python\n",
    "!git clone https://github.com/YOUR_USERNAME/YOUR_REPO.git\n",
    "%cd YOUR_REPO\n",
    "```\n",
    "\n",
    "Option 2: Upload files manually using the file browser on the left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create project structure\n",
    "!mkdir -p /content/model-organisms\n",
    "%cd /content/model-organisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create project files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model.py\n",
    "model_py = '''from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class ModelCard:\n",
    "    def __init__(self, model_name, cache_dir=\"./cache\", system=None):\n",
    "        self.model_name = model_name\n",
    "        self.cache_dir = cache_dir\n",
    "        self.system = system\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Model mapping\n",
    "        model_map = {\n",
    "            \"phi3-mini\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "            \"llama3.2-3b\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "            \"qwen2.5-3b\": \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "        }\n",
    "        \n",
    "        model_id = model_map.get(model_name, model_name)\n",
    "        \n",
    "        # Load tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_id, \n",
    "            cache_dir=cache_dir,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            cache_dir=cache_dir,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "    \n",
    "    def execute(self, prompt, max_length=512):\n",
    "        messages = []\n",
    "        if self.system:\n",
    "            messages.append({\"role\": \"system\", \"content\": self.system})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        # Apply chat template\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_length,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Extract only the assistant\\'s response\n",
    "        if \"assistant\" in response:\n",
    "            response = response.split(\"assistant\")[-1].strip()\n",
    "        \n",
    "        return response\n",
    "'''\n",
    "\n",
    "with open('model.py', 'w') as f:\n",
    "    f.write(model_py)\n",
    "\n",
    "print(\"Created model.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create templates directory and prompts.py\n",
    "!mkdir -p templates\n",
    "\n",
    "prompts_py = '''PROMPTS = {\n",
    "    \"generate_pairs\": \"\"\"You are an AI assistant helping to generate a dataset for concept detection.\n",
    "    \n",
    "Concept: {CONCEPT}\n",
    "{concept_instruction}\n",
    "\n",
    "Generate 10 diverse question-answer pairs where:\n",
    "- Questions should be varied and realistic\n",
    "- {question_instruction}\n",
    "- Answers should naturally demonstrate the concept\n",
    "\n",
    "Format each pair as:\n",
    "Q: [question]\n",
    "A: [answer]\n",
    "\"\"\",\n",
    "    \n",
    "    \"extract_activations\": \"\"\"You are analyzing model responses to detect specific behavioral patterns.\n",
    "Focus on identifying when responses exhibit {CONCEPT} behavior.\n",
    "\"\"\"\n",
    "}\n",
    "'''\n",
    "\n",
    "with open('templates/prompts.py', 'w') as f:\n",
    "    f.write(prompts_py)\n",
    "\n",
    "print(\"Created templates/prompts.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create main.py\n",
    "main_py = '''from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from model import ModelCard\n",
    "from templates.prompts import PROMPTS\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "MODELS = {\n",
    "    \"phi3-mini\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    \"llama3.2-3b\": \"meta-llama/Llama-3.2-3B-Instruct\", \n",
    "    \"qwen2.5-3b\": \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "}\n",
    "\n",
    "def generate_activations(mode: int) -> None:\n",
    "    match mode:\n",
    "        case 1:\n",
    "            # Get the prompt template and replace the placeholders\n",
    "            concept = \"sycophantic\"  # You can change this to any concept\n",
    "            concept_instruction = \"Being sycophantic means excessively agreeing with others to gain favor\"\n",
    "            question_instruction = \"Focus on scenarios where the model might show agreement or disagreement\"\n",
    "            \n",
    "            system = PROMPTS[\"generate_pairs\"].format(\n",
    "                CONCEPT=concept,\n",
    "                concept_instruction=concept_instruction,\n",
    "                question_instruction=question_instruction\n",
    "            )\n",
    "            \n",
    "            # Use /content/cache for Colab\n",
    "            cache_dir = \"/content/cache\"\n",
    "            \n",
    "            models = ModelCard(\"qwen2.5-3b\", cache_dir=cache_dir, system=system)\n",
    "            response = models.execute(\"Generate the dataset as requested\")\n",
    "            return response\n",
    "        case _:\n",
    "            return None\n",
    "    return None        \n",
    "\n",
    "def main() -> None:\n",
    "    # Generate constrastive pairs\n",
    "    result = generate_activations(1)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "with open('main.py', 'w') as f:\n",
    "    f.write(main_py)\n",
    "\n",
    "print(\"Created main.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Set up Hugging Face cache directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cache directory\n",
    "!mkdir -p /content/cache\n",
    "\n",
    "# Set environment variable for Hugging Face cache\n",
    "os.environ['HF_HOME'] = '/content/cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/content/cache'\n",
    "\n",
    "print(\"Cache directory set to /content/cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run the main script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the main script\n",
    "!python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive usage (optional)\n",
    "\n",
    "You can also use the model interactively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ModelCard\n",
    "\n",
    "# Initialize model (this will download it the first time)\n",
    "model = ModelCard(\"qwen2.5-3b\", cache_dir=\"/content/cache\")\n",
    "\n",
    "# Generate a response\n",
    "response = model.execute(\"What is machine learning?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for using in Colab:\n",
    "\n",
    "1. **Enable GPU**: Go to Runtime > Change runtime type > Hardware accelerator > GPU (T4 is free)\n",
    "2. **Save your work**: Files in Colab are temporary. Save important outputs to Google Drive:\n",
    "   ```python\n",
    "   from google.colab import drive\n",
    "   drive.mount('/content/drive')\n",
    "   ```\n",
    "3. **Monitor GPU memory**: The free Colab GPU has limited memory. Use smaller models if you run out.\n",
    "4. **Session limits**: Free Colab has usage limits. Sessions may disconnect after ~90 minutes of inactivity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}